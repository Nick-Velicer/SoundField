{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 13:36:34.476298: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-20 13:36:34.876712: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-20 13:36:48.653685: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-20 13:36:48.655595: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-20 13:36:48.655605: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# imports numpy, pandas, matplotlib, scikit-learn, tensorflow needed\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_subj = 2\n",
    "\n",
    "# num_games = [4]\n",
    "def FunctionGetSubData(n_subj, n_game): \n",
    "    n = 1\n",
    "    while n <= n_subj:\n",
    "        if n < 10: \n",
    "            emotions = \"GAMEEMO/(S0{})/Preprocessed EEG Data/.csv format/S0{}G{}AllChannels.csv\".format(n, n, n_game)\n",
    "        else:\n",
    "            emotions = \"GAMEEMO/(S{})/Preprocessed EEG Data/.csv format/S{}G{}AllChannels.csv\".format(n, n, n_game)\n",
    "        data = pd.read_csv(emotions)\n",
    "        data = data.drop(columns=[\"Unnamed: 14\"])\n",
    "        data[\"subject_num\"] = n\n",
    "        \n",
    "        if n == 1:\n",
    "            emo_df = data.iloc[15300:30600]\n",
    "        else:\n",
    "            # third minute of each subject's data\n",
    "            emo_df2 = data.iloc[15300:30600]\n",
    "            emo_df = pd.concat([emo_df, emo_df2], axis=0)\n",
    "        # emotions1= emotions1.reset_index(drop=True)\n",
    "        n = n+1\n",
    "    \n",
    "    # emotions[\"pred_valence\"] = 1\n",
    "    # emotions[\"pred_arousal\"] = 1\n",
    "    return (emo_df)\n",
    "    # print(emotions_funny.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FunctionGetGameData(n_subj, n_games):\n",
    "    for game in n_games:\n",
    "        # # 1 = bored (0,0); 2 = calm (1,0); 3 = horror (0,1); 4 = funny (1,1)\n",
    "        if game == 1:\n",
    "            emo_bored = FunctionGetSubData(n_subj, game)\n",
    "            emo_bored[\"pred_valence\"] = 0\n",
    "            emo_bored[\"pred_arousal\"] = 0\n",
    "        elif game == 2:\n",
    "            emo_calm = FunctionGetSubData(n_subj, game)\n",
    "            emo_calm[\"pred_valence\"] = 1\n",
    "            emo_calm[\"pred_arousal\"] = 0\n",
    "        elif game == 3:\n",
    "            emo_horror = FunctionGetSubData(n_subj, game)\n",
    "            emo_horror[\"pred_valence\"] = 0\n",
    "            emo_horror[\"pred_arousal\"] = 1\n",
    "        elif game == 4:\n",
    "            emo_funny = FunctionGetSubData(n_subj, game)\n",
    "            emo_funny[\"pred_valence\"] = 1\n",
    "            emo_funny[\"pred_arousal\"] = 1\n",
    "    emotions_all = pd.concat([emo_bored, emo_calm, emo_horror, emo_funny], axis=0)\n",
    "    return emotions_all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            AF3       AF4       F3         F4        F7       F8      FC5  \\\n",
      "15300  -9.39300   9.40330   7.3046   6.108800 -11.45820  22.6112   7.6865   \n",
      "15301  -6.41920   7.15210  14.3379   3.934500   3.32930  13.4209  21.3253   \n",
      "15302   6.25820   0.21479   6.4875   0.083213  17.22950   4.6619   5.9458   \n",
      "15303  -8.21730 -13.89810  -8.0017 -14.021800   2.09570  -9.7179  -8.5109   \n",
      "15304 -12.66530 -21.37930 -13.9085 -27.280500  -2.24810 -14.5578  -2.9388   \n",
      "...         ...       ...      ...        ...       ...      ...      ...   \n",
      "30595  -4.88140  -5.29560  -6.5941  -0.063464   0.61872  -6.8075   9.1474   \n",
      "30596  -1.93740  -9.55710 -10.2963  -3.675100   2.26880  -0.3735  10.2858   \n",
      "30597   0.58922  -9.46590 -10.6426  -8.275100   4.06070  -4.2077   5.3301   \n",
      "30598  -2.57990  -7.21090  -7.8347  -7.055400   5.02210 -10.4630   5.7330   \n",
      "30599  -1.94270  -7.74210  -5.9183  -4.703900  10.98770  -5.9783  13.1022   \n",
      "\n",
      "            FC6       O1       O2       P7       P8        T7       T8  \\\n",
      "15300   0.34389  10.3142   5.3013  12.6977   5.7506   3.90800  -1.0627   \n",
      "15301 -13.77670   7.4056  -9.1168  -2.1642  11.4313 -10.42650   4.0625   \n",
      "15302   1.14990   1.8998   5.5302  12.0657  12.9144   4.29910  -0.2788   \n",
      "15303  15.18090 -12.3142  19.2984  25.4417  -1.9605   1.99250 -14.3621   \n",
      "15304   7.76230  -6.3933  27.0583  38.0152   5.7496   4.28290  -8.8002   \n",
      "...         ...      ...      ...      ...      ...       ...      ...   \n",
      "30595  -3.21000   9.0997  -8.6605 -17.5299 -41.0532 -10.02700  -7.7247   \n",
      "30596  -5.18650  10.2409  -5.0078 -17.2012 -32.5645  -6.29210  -8.9485   \n",
      "30597  -4.87560   7.2161  -5.1895 -15.2050 -28.2005  -2.05820  -9.3756   \n",
      "30598  -4.82420   4.1318 -11.8678 -16.9441 -28.1957  -1.21160  -8.0900   \n",
      "30599  -4.53430   9.6690 -13.5659 -15.9272 -26.0215  -0.17456  -7.6046   \n",
      "\n",
      "       subject_num  pred_valence  pred_arousal  \n",
      "15300            1             0             0  \n",
      "15301            1             0             0  \n",
      "15302            1             0             0  \n",
      "15303            1             0             0  \n",
      "15304            1             0             0  \n",
      "...            ...           ...           ...  \n",
      "30595            8             1             1  \n",
      "30596            8             1             1  \n",
      "30597            8             1             1  \n",
      "30598            8             1             1  \n",
      "30599            8             1             1  \n",
      "\n",
      "[489600 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "# only using first 8 subjects, all four games\n",
    "emo_all = FunctionGetGameData(8, [1, 2, 3, 4])\n",
    "print(emo_all)\n",
    "\n",
    "# emo_bored = FunctionGetSubData(6, 1)\n",
    "# print(emo_bored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(342720, 14)\n",
      "(342720, 2)\n",
      "(146880, 14)\n",
      "(146880, 2)\n"
     ]
    }
   ],
   "source": [
    "# Separate Target Variable and Predictor Variables\n",
    "TargetVariable=['pred_valence', 'pred_arousal']\n",
    "# Predictors=['AF3', 'AF4', 'F3', 'F4']\n",
    "Predictors=['AF3', 'AF4', 'F3', 'F4', 'F7', 'F8' ,'FC5', 'FC6', 'O1', 'O2', 'P7', 'P8', 'T7', 'T8']\n",
    "X=emo_all[Predictors].values\n",
    "y=emo_all[TargetVariable].values\n",
    "\n",
    "### Sandardization of data ###\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "PredictorScaler=StandardScaler()\n",
    "TargetVarScaler=StandardScaler()\n",
    "\n",
    "# Storing the fit object for later reference\n",
    "PredictorScalerFit=PredictorScaler.fit(X)\n",
    "TargetVarScalerFit=TargetVarScaler.fit(y)\n",
    "\n",
    "# Generating the standardized values of X and y\n",
    "X=PredictorScalerFit.transform(X)\n",
    "y=TargetVarScalerFit.transform(y)\n",
    "\n",
    "# Split the data into training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Quick sanity check with the shapes of Training and testing datasets\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to find the best parameters for ANN\n",
    "def FunctionFindBestParams(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Defining the list of hyper parameters to try\n",
    "    batch_size_list=[10, 25, 50]\n",
    "    epoch_list  =   [10, 20, 30]\n",
    "    \n",
    "    import pandas as pd\n",
    "    SearchResultsData=pd.DataFrame(columns=['TrialNumber', 'Parameters', 'Accuracy'])\n",
    "    \n",
    "    # initializing the trials\n",
    "    TrialNumber=0\n",
    "    for batch_size_trial in batch_size_list:\n",
    "        for epochs_trial in epoch_list:\n",
    "            TrialNumber+=1\n",
    "            # create ANN model\n",
    "            model = Sequential()\n",
    "            # Defining the first layer of the model\n",
    "            model.add(Dense(units=10, input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    " \n",
    "            # applying dropout to layer\n",
    "            model.add(Dropout(0.1))\n",
    "\n",
    "            # Defining the Second layer of the model\n",
    "            model.add(Dense(units=10, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "            # Defining the third layer of the model\n",
    "            model.add(Dense(units=5, kernel_initializer='normal', activation='relu'))\n",
    " \n",
    "            # applying dropout to layer\n",
    "            model.add(Dropout(0.1))\n",
    "\n",
    "            # The output neuron is a single fully connected node \n",
    "            # Since we will be predicting a single number\n",
    "            model.add(Dense(2, kernel_initializer='normal'))\n",
    " \n",
    "            # Compiling the model\n",
    "            model.compile(loss='mean_squared_error', optimizer='adam')\n",
    " \n",
    "            # Fitting the ANN to the Training set\n",
    "            model.fit(X_train, y_train ,batch_size = batch_size_trial, epochs = epochs_trial, verbose=2)\n",
    " \n",
    "            # Mean absolute percentage error\n",
    "            MAPE = mean_absolute_percentage_error(y_test, model.predict(X_test))\n",
    "            \n",
    "            # printing the results of the current iteration\n",
    "            print(TrialNumber, 'Parameters:','batch_size:', batch_size_trial,'-', 'epochs:',epochs_trial, 'Accuracy:', 100-MAPE)\n",
    "            \n",
    "            SearchResultsData=SearchResultsData.append(pd.DataFrame(data=[[TrialNumber, str(batch_size_trial)+'-'+str(epochs_trial), 100-MAPE]],\n",
    "                                                                    columns=['TrialNumber', 'Parameters', 'Accuracy'] ))\n",
    "    return(SearchResultsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the function\n",
    "ResultsData =FunctionFindBestParams(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "%matplotlib inline\n",
    "ResultsData.plot(x='Parameters', y='Accuracy', figsize=(15,4), kind='line')\n",
    "# 25, 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "\n",
    "def functionBestModel(batch, epoch):\n",
    "    model = Sequential()\n",
    "    # Defining the first layer of the model\n",
    "    model.add(Dense(units=10, input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "    # applying dropout to layer\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    # Defining the Second layer of the model\n",
    "    model.add(Dense(units=10, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "    # Defining the third layer of the model\n",
    "    model.add(Dense(units=5, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "    # applying dropout to layer\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    # The output neuron is a single fully connected node \n",
    "    # Since we will be predicting a single number\n",
    "    model.add(Dense(2, kernel_initializer='normal'))\n",
    "\n",
    "    # Compiling the model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    # Fitting the ANN to the Training set\n",
    "    model.fit(X_train, y_train ,batch_size = batch, epochs = epoch, callbacks=[cp_callback], verbose=2)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 13:38:39.887463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 13:38:39.958146: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-03-20 13:38:39.958183: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-03-20 13:38:39.959654: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\n",
      "Epoch 1: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 11s - loss: 0.9651 - 11s/epoch - 815us/step\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 2: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 12s - loss: 0.9542 - 12s/epoch - 843us/step\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 3: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 10s - loss: 0.9496 - 10s/epoch - 763us/step\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 4: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 10s - loss: 0.9467 - 10s/epoch - 760us/step\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 5: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 12s - loss: 0.9452 - 12s/epoch - 883us/step\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 6: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 11s - loss: 0.9444 - 11s/epoch - 766us/step\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 7: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 10s - loss: 0.9440 - 10s/epoch - 739us/step\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 8: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 10s - loss: 0.9437 - 10s/epoch - 713us/step\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 9: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 10s - loss: 0.9428 - 10s/epoch - 757us/step\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 10: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 11s - loss: 0.9430 - 11s/epoch - 795us/step\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 11: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 11s - loss: 0.9430 - 11s/epoch - 791us/step\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 12: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 11s - loss: 0.9429 - 11s/epoch - 787us/step\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 13: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 11s - loss: 0.9424 - 11s/epoch - 803us/step\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 14: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 10s - loss: 0.9421 - 10s/epoch - 751us/step\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 15: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 10s - loss: 0.9424 - 10s/epoch - 741us/step\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 16: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 10s - loss: 0.9424 - 10s/epoch - 761us/step\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 17: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 11s - loss: 0.9415 - 11s/epoch - 778us/step\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 18: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 11s - loss: 0.9421 - 11s/epoch - 784us/step\n",
      "Epoch 19/50\n",
      "\n",
      "Epoch 19: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 10s - loss: 0.9421 - 10s/epoch - 765us/step\n",
      "Epoch 20/50\n",
      "\n",
      "Epoch 20: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 10s - loss: 0.9415 - 10s/epoch - 763us/step\n",
      "Epoch 21/50\n",
      "\n",
      "Epoch 21: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 11s - loss: 0.9416 - 11s/epoch - 774us/step\n",
      "Epoch 22/50\n",
      "\n",
      "Epoch 22: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 10s - loss: 0.9418 - 10s/epoch - 749us/step\n",
      "Epoch 23/50\n",
      "\n",
      "Epoch 23: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 10s - loss: 0.9411 - 10s/epoch - 761us/step\n",
      "Epoch 24/50\n",
      "\n",
      "Epoch 24: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 12s - loss: 0.9413 - 12s/epoch - 895us/step\n",
      "Epoch 25/50\n",
      "\n",
      "Epoch 25: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 11s - loss: 0.9409 - 11s/epoch - 814us/step\n",
      "Epoch 26/50\n",
      "\n",
      "Epoch 26: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 11s - loss: 0.9406 - 11s/epoch - 789us/step\n",
      "Epoch 27/50\n",
      "\n",
      "Epoch 27: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 11s - loss: 0.9412 - 11s/epoch - 780us/step\n",
      "Epoch 28/50\n",
      "\n",
      "Epoch 28: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 11s - loss: 0.9405 - 11s/epoch - 768us/step\n",
      "Epoch 29/50\n",
      "\n",
      "Epoch 29: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 11s - loss: 0.9403 - 11s/epoch - 802us/step\n",
      "Epoch 30/50\n",
      "\n",
      "Epoch 30: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 11s - loss: 0.9403 - 11s/epoch - 769us/step\n",
      "Epoch 31/50\n",
      "\n",
      "Epoch 31: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 10s - loss: 0.9406 - 10s/epoch - 745us/step\n",
      "Epoch 32/50\n",
      "\n",
      "Epoch 32: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 10s - loss: 0.9408 - 10s/epoch - 754us/step\n",
      "Epoch 33/50\n",
      "\n",
      "Epoch 33: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 10s - loss: 0.9406 - 10s/epoch - 757us/step\n",
      "Epoch 34/50\n",
      "\n",
      "Epoch 34: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 11s - loss: 0.9406 - 11s/epoch - 784us/step\n",
      "Epoch 35/50\n",
      "\n",
      "Epoch 35: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 11s - loss: 0.9403 - 11s/epoch - 799us/step\n",
      "Epoch 36/50\n",
      "\n",
      "Epoch 36: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 12s - loss: 0.9401 - 12s/epoch - 842us/step\n",
      "Epoch 37/50\n",
      "\n",
      "Epoch 37: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 11s - loss: 0.9397 - 11s/epoch - 824us/step\n",
      "Epoch 38/50\n",
      "\n",
      "Epoch 38: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 11s - loss: 0.9401 - 11s/epoch - 803us/step\n",
      "Epoch 39/50\n",
      "\n",
      "Epoch 39: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 11s - loss: 0.9396 - 11s/epoch - 812us/step\n",
      "Epoch 40/50\n",
      "\n",
      "Epoch 40: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 11s - loss: 0.9402 - 11s/epoch - 795us/step\n",
      "Epoch 41/50\n",
      "\n",
      "Epoch 41: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 11s - loss: 0.9398 - 11s/epoch - 801us/step\n",
      "Epoch 42/50\n",
      "\n",
      "Epoch 42: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 11s - loss: 0.9399 - 11s/epoch - 801us/step\n",
      "Epoch 43/50\n",
      "\n",
      "Epoch 43: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 11s - loss: 0.9396 - 11s/epoch - 799us/step\n",
      "Epoch 44/50\n",
      "\n",
      "Epoch 44: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 11s - loss: 0.9394 - 11s/epoch - 794us/step\n",
      "Epoch 45/50\n",
      "\n",
      "Epoch 45: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 12s - loss: 0.9396 - 12s/epoch - 848us/step\n",
      "Epoch 46/50\n",
      "\n",
      "Epoch 46: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 11s - loss: 0.9400 - 11s/epoch - 804us/step\n",
      "Epoch 47/50\n",
      "\n",
      "Epoch 47: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 11s - loss: 0.9401 - 11s/epoch - 807us/step\n",
      "Epoch 48/50\n",
      "\n",
      "Epoch 48: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 11s - loss: 0.9404 - 11s/epoch - 819us/step\n",
      "Epoch 49/50\n",
      "\n",
      "Epoch 49: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 10s - loss: 0.9392 - 10s/epoch - 761us/step\n",
      "Epoch 50/50\n",
      "\n",
      "Epoch 50: saving model to training_1/cp.ckpt\n",
      "13709/13709 - 12s - loss: 0.9404 - 12s/epoch - 861us/step\n"
     ]
    }
   ],
   "source": [
    "model = functionBestModel(25, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4590/4590 [==============================] - 3s 609us/step\n"
     ]
    }
   ],
   "source": [
    "# Generating Predictions on testing data\n",
    "Predictions=model.predict(X_test)\n",
    " \n",
    "# Scaling the predicted Price data back to original price scale\n",
    "Predictions=TargetVarScalerFit.inverse_transform(Predictions)\n",
    " \n",
    "# Scaling the y_test Price data back to original price scale\n",
    "y_test_orig=TargetVarScalerFit.inverse_transform(y_test)\n",
    " \n",
    "# Scaling the test data back to original scale\n",
    "Test_Data=PredictorScalerFit.inverse_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AF3</th>\n",
       "      <th>AF4</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>FC5</th>\n",
       "      <th>FC6</th>\n",
       "      <th>O1</th>\n",
       "      <th>O2</th>\n",
       "      <th>P7</th>\n",
       "      <th>P8</th>\n",
       "      <th>T7</th>\n",
       "      <th>T8</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Arousal</th>\n",
       "      <th>PredictedValence</th>\n",
       "      <th>PredictedArousal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.0482</td>\n",
       "      <td>7.8587</td>\n",
       "      <td>-5.9801</td>\n",
       "      <td>-4.13170</td>\n",
       "      <td>-4.61150</td>\n",
       "      <td>37.3873</td>\n",
       "      <td>-4.6919</td>\n",
       "      <td>-2.0376</td>\n",
       "      <td>-9.8944</td>\n",
       "      <td>-10.7469</td>\n",
       "      <td>14.56240</td>\n",
       "      <td>-12.5265</td>\n",
       "      <td>2.7649</td>\n",
       "      <td>5.1429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.503212</td>\n",
       "      <td>0.502107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-8.3637</td>\n",
       "      <td>2.5503</td>\n",
       "      <td>-2.8147</td>\n",
       "      <td>-0.53312</td>\n",
       "      <td>0.23146</td>\n",
       "      <td>-0.7941</td>\n",
       "      <td>-3.5293</td>\n",
       "      <td>3.8227</td>\n",
       "      <td>9.3997</td>\n",
       "      <td>12.1075</td>\n",
       "      <td>11.06240</td>\n",
       "      <td>8.3727</td>\n",
       "      <td>8.6241</td>\n",
       "      <td>2.1006</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.584945</td>\n",
       "      <td>0.638189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.0151</td>\n",
       "      <td>-2.2099</td>\n",
       "      <td>-10.2183</td>\n",
       "      <td>-3.61710</td>\n",
       "      <td>2.08490</td>\n",
       "      <td>-33.7221</td>\n",
       "      <td>8.3567</td>\n",
       "      <td>-42.0455</td>\n",
       "      <td>-24.2231</td>\n",
       "      <td>-6.9765</td>\n",
       "      <td>23.46250</td>\n",
       "      <td>-3.0187</td>\n",
       "      <td>10.4903</td>\n",
       "      <td>-1.8339</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.495155</td>\n",
       "      <td>0.335442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-6.6439</td>\n",
       "      <td>-1.4744</td>\n",
       "      <td>-8.8610</td>\n",
       "      <td>3.48690</td>\n",
       "      <td>-0.74980</td>\n",
       "      <td>-10.5614</td>\n",
       "      <td>-2.2874</td>\n",
       "      <td>6.5161</td>\n",
       "      <td>1.4673</td>\n",
       "      <td>5.9673</td>\n",
       "      <td>-0.33357</td>\n",
       "      <td>-1.5761</td>\n",
       "      <td>-0.6879</td>\n",
       "      <td>-6.6033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.501175</td>\n",
       "      <td>0.489526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.9313</td>\n",
       "      <td>5.6357</td>\n",
       "      <td>-5.4788</td>\n",
       "      <td>-8.48180</td>\n",
       "      <td>11.53590</td>\n",
       "      <td>17.4337</td>\n",
       "      <td>-1.5928</td>\n",
       "      <td>-4.8819</td>\n",
       "      <td>-2.0763</td>\n",
       "      <td>-40.0687</td>\n",
       "      <td>7.07360</td>\n",
       "      <td>-9.1306</td>\n",
       "      <td>-3.8230</td>\n",
       "      <td>-35.1435</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.667146</td>\n",
       "      <td>0.693877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       AF3     AF4       F3       F4        F7       F8     FC5      FC6  \\\n",
       "0  -3.0482  7.8587  -5.9801 -4.13170  -4.61150  37.3873 -4.6919  -2.0376   \n",
       "1  -8.3637  2.5503  -2.8147 -0.53312   0.23146  -0.7941 -3.5293   3.8227   \n",
       "2  12.0151 -2.2099 -10.2183 -3.61710   2.08490 -33.7221  8.3567 -42.0455   \n",
       "3  -6.6439 -1.4744  -8.8610  3.48690  -0.74980 -10.5614 -2.2874   6.5161   \n",
       "4  10.9313  5.6357  -5.4788 -8.48180  11.53590  17.4337 -1.5928  -4.8819   \n",
       "\n",
       "        O1       O2        P7       P8       T7       T8  Valence  Arousal  \\\n",
       "0  -9.8944 -10.7469  14.56240 -12.5265   2.7649   5.1429      1.0      0.0   \n",
       "1   9.3997  12.1075  11.06240   8.3727   8.6241   2.1006      1.0      1.0   \n",
       "2 -24.2231  -6.9765  23.46250  -3.0187  10.4903  -1.8339      1.0      0.0   \n",
       "3   1.4673   5.9673  -0.33357  -1.5761  -0.6879  -6.6033      0.0      1.0   \n",
       "4  -2.0763 -40.0687   7.07360  -9.1306  -3.8230 -35.1435      1.0      0.0   \n",
       "\n",
       "   PredictedValence  PredictedArousal  \n",
       "0          0.503212          0.502107  \n",
       "1          0.584945          0.638189  \n",
       "2          0.495155          0.335442  \n",
       "3          0.501175          0.489526  \n",
       "4          0.667146          0.693877  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TestingData=pd.DataFrame(data=Test_Data, columns=Predictors)\n",
    "TestingData[['Valence', 'Arousal']]=y_test_orig\n",
    "TestingData[['PredictedValence', 'PredictedArousal']]=Predictions\n",
    "TestingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model in predicting valence is:  63.44309565450891\n",
      "Accuracy of model in predicting arousal is:  66.04787673730168\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AF3</th>\n",
       "      <th>AF4</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>FC5</th>\n",
       "      <th>FC6</th>\n",
       "      <th>O1</th>\n",
       "      <th>O2</th>\n",
       "      <th>P7</th>\n",
       "      <th>P8</th>\n",
       "      <th>T7</th>\n",
       "      <th>T8</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Arousal</th>\n",
       "      <th>PredictedValence</th>\n",
       "      <th>PredictedArousal</th>\n",
       "      <th>APE_val</th>\n",
       "      <th>APE_aro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.0482</td>\n",
       "      <td>7.8587</td>\n",
       "      <td>-5.9801</td>\n",
       "      <td>-4.13170</td>\n",
       "      <td>-4.61150</td>\n",
       "      <td>37.3873</td>\n",
       "      <td>-4.6919</td>\n",
       "      <td>-2.0376</td>\n",
       "      <td>-9.8944</td>\n",
       "      <td>-10.7469</td>\n",
       "      <td>14.56240</td>\n",
       "      <td>-12.5265</td>\n",
       "      <td>2.7649</td>\n",
       "      <td>5.1429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.503212</td>\n",
       "      <td>0.502107</td>\n",
       "      <td>24.839425</td>\n",
       "      <td>50.210738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-8.3637</td>\n",
       "      <td>2.5503</td>\n",
       "      <td>-2.8147</td>\n",
       "      <td>-0.53312</td>\n",
       "      <td>0.23146</td>\n",
       "      <td>-0.7941</td>\n",
       "      <td>-3.5293</td>\n",
       "      <td>3.8227</td>\n",
       "      <td>9.3997</td>\n",
       "      <td>12.1075</td>\n",
       "      <td>11.06240</td>\n",
       "      <td>8.3727</td>\n",
       "      <td>8.6241</td>\n",
       "      <td>2.1006</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.584945</td>\n",
       "      <td>0.638189</td>\n",
       "      <td>20.752758</td>\n",
       "      <td>18.090546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.0151</td>\n",
       "      <td>-2.2099</td>\n",
       "      <td>-10.2183</td>\n",
       "      <td>-3.61710</td>\n",
       "      <td>2.08490</td>\n",
       "      <td>-33.7221</td>\n",
       "      <td>8.3567</td>\n",
       "      <td>-42.0455</td>\n",
       "      <td>-24.2231</td>\n",
       "      <td>-6.9765</td>\n",
       "      <td>23.46250</td>\n",
       "      <td>-3.0187</td>\n",
       "      <td>10.4903</td>\n",
       "      <td>-1.8339</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.495155</td>\n",
       "      <td>0.335442</td>\n",
       "      <td>25.242269</td>\n",
       "      <td>33.544183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-6.6439</td>\n",
       "      <td>-1.4744</td>\n",
       "      <td>-8.8610</td>\n",
       "      <td>3.48690</td>\n",
       "      <td>-0.74980</td>\n",
       "      <td>-10.5614</td>\n",
       "      <td>-2.2874</td>\n",
       "      <td>6.5161</td>\n",
       "      <td>1.4673</td>\n",
       "      <td>5.9673</td>\n",
       "      <td>-0.33357</td>\n",
       "      <td>-1.5761</td>\n",
       "      <td>-0.6879</td>\n",
       "      <td>-6.6033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.501175</td>\n",
       "      <td>0.489526</td>\n",
       "      <td>50.117540</td>\n",
       "      <td>25.523686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.9313</td>\n",
       "      <td>5.6357</td>\n",
       "      <td>-5.4788</td>\n",
       "      <td>-8.48180</td>\n",
       "      <td>11.53590</td>\n",
       "      <td>17.4337</td>\n",
       "      <td>-1.5928</td>\n",
       "      <td>-4.8819</td>\n",
       "      <td>-2.0763</td>\n",
       "      <td>-40.0687</td>\n",
       "      <td>7.07360</td>\n",
       "      <td>-9.1306</td>\n",
       "      <td>-3.8230</td>\n",
       "      <td>-35.1435</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.667146</td>\n",
       "      <td>0.693877</td>\n",
       "      <td>16.642702</td>\n",
       "      <td>69.387674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       AF3     AF4       F3       F4        F7       F8     FC5      FC6  \\\n",
       "0  -3.0482  7.8587  -5.9801 -4.13170  -4.61150  37.3873 -4.6919  -2.0376   \n",
       "1  -8.3637  2.5503  -2.8147 -0.53312   0.23146  -0.7941 -3.5293   3.8227   \n",
       "2  12.0151 -2.2099 -10.2183 -3.61710   2.08490 -33.7221  8.3567 -42.0455   \n",
       "3  -6.6439 -1.4744  -8.8610  3.48690  -0.74980 -10.5614 -2.2874   6.5161   \n",
       "4  10.9313  5.6357  -5.4788 -8.48180  11.53590  17.4337 -1.5928  -4.8819   \n",
       "\n",
       "        O1       O2        P7       P8       T7       T8  Valence  Arousal  \\\n",
       "0  -9.8944 -10.7469  14.56240 -12.5265   2.7649   5.1429      1.0      0.0   \n",
       "1   9.3997  12.1075  11.06240   8.3727   8.6241   2.1006      1.0      1.0   \n",
       "2 -24.2231  -6.9765  23.46250  -3.0187  10.4903  -1.8339      1.0      0.0   \n",
       "3   1.4673   5.9673  -0.33357  -1.5761  -0.6879  -6.6033      0.0      1.0   \n",
       "4  -2.0763 -40.0687   7.07360  -9.1306  -3.8230 -35.1435      1.0      0.0   \n",
       "\n",
       "   PredictedValence  PredictedArousal    APE_val    APE_aro  \n",
       "0          0.503212          0.502107  24.839425  50.210738  \n",
       "1          0.584945          0.638189  20.752758  18.090546  \n",
       "2          0.495155          0.335442  25.242269  33.544183  \n",
       "3          0.501175          0.489526  50.117540  25.523686  \n",
       "4          0.667146          0.693877  16.642702  69.387674  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APE_val = 100*(abs((TestingData['Valence']+1)-(TestingData['PredictedValence']+1))/(TestingData['Valence']+1))\n",
    "TestingData['APE_val'] = APE_val\n",
    "\n",
    "APE_aro = 100*(abs((TestingData['Arousal']+1)-(TestingData['PredictedArousal']+1))/(TestingData['Arousal']+1))\n",
    "TestingData['APE_aro'] = APE_aro\n",
    "\n",
    "print('Accuracy of model in predicting valence is: ', 100-np.mean(APE_val))\n",
    "print('Accuracy of model in predicting arousal is: ', 100-np.mean(APE_aro))\n",
    "\n",
    "TestingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 10)                150       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 10)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 12        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 327\n",
      "Trainable params: 327\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the weights\n",
    "# model.load_weights(checkpoint_path)\n",
    "\n",
    "# # Re-evaluate the model\n",
    "# loss, acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "# print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    92966\n",
       "True     53914\n",
       "Name: Match, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 = bored (0,0); 2 = calm (1,0); 3 = horror (0,1); 4 = funny (1,1)\n",
    "# def functionFindEmo(data):\n",
    "\n",
    "pred_data = TestingData[['Valence', 'Arousal', 'PredictedValence', 'PredictedArousal',]].copy()\n",
    "\n",
    "pred_data.loc[(pred_data['PredictedValence'] > 0.5) & (pred_data['PredictedArousal'] > 0.5), 'PredEmotionClass'] = 4\n",
    "pred_data.loc[(pred_data['PredictedValence'] < 0.5) & (pred_data['PredictedArousal'] > 0.5), 'PredEmotionClass'] = 3\n",
    "pred_data.loc[(pred_data['PredictedValence'] > 0.5) & (pred_data['PredictedArousal'] < 0.5), 'PredEmotionClass'] = 2\n",
    "pred_data.loc[(pred_data['PredictedValence'] < 0.5) & (pred_data['PredictedArousal'] < 0.5), 'PredEmotionClass'] = 1\n",
    "\n",
    "pred_data.loc[(pred_data['Valence'] > 0.5) & (pred_data['Arousal'] > 0.5), 'RealEmotionClass'] = 4\n",
    "pred_data.loc[(pred_data['Valence'] < 0.5) & (pred_data['Arousal'] > 0.5), 'RealEmotionClass'] = 3\n",
    "pred_data.loc[(pred_data['Valence'] > 0.5) & (pred_data['Arousal'] < 0.5), 'RealEmotionClass'] = 2\n",
    "pred_data.loc[(pred_data['Valence'] < 0.5) & (pred_data['Arousal'] < 0.5), 'RealEmotionClass'] = 1\n",
    "\n",
    "pred_data.loc[(pred_data['PredEmotionClass'] == pred_data['RealEmotionClass']), 'Match'] = 'True'\n",
    "pred_data.loc[(pred_data['PredEmotionClass'] != pred_data['RealEmotionClass']), 'Match'] = 'False'\n",
    "\n",
    "\n",
    "pred_data.head()  \n",
    "pred_data.Match.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
