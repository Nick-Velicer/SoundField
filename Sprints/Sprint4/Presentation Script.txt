p;l;Intro: Have you ever wondered what is actually going on in your brain when you listen to your favorite song? Or even your least favorite song? Or what that could look like as a piece of art? Hi, we’re team 17. *everyone introduces themselves*. For our project, Soundfield we’re designing an application that aims to visualize just what this might look like. The program is a marriage of Code, Art, and Neuroscience where we use generative art to make unique creations based on a user’s brain activity when listening to different kinds of music. We’ll take input from an electroencephalogram, or EEG, headset while someone is listening to music, transfer the data to a database, and then run it through our software to create artwork based on their thought and emotional patterns. For this project, we are using an EMOTIV EEG headset, python for our algorithms, and HTML and CSS for the web page creation, but more on that later. 
Electroencephalography, or EEG, is a method of recording neurological activity. In order to record this activity, a special headset is worn. This headset includes sensors, known as electrodes, that detect and record the electrical activity of neurons firing in the brain. EEG electrodes detect the summation of multiple neurons firing at once in a given region through the process of constructive interference. When someone feels, thinks, or moves, different parts of the brain fire electrical signals in order to carry out that thought, emotion, or movement. The headset gathers information on this activity by measuring these electrical signals.
There are a number of factors that can influence the quality of an EEG recording. When gathering data as precise as neural impulses, you have to consider a number of sources of possible noise. Obviously, when recording data directly from the brain, it is difficult to maintain a controlled, consistent environment. Any number of stimuli could cause distractions in the subjects, resulting in noisy data. Even something as simple as a blink produces an incredibly significant imprint in the collected data. Something as minute as noise from nearby electronics can greatly impact the quality of EEG data. In order to control for each and every one of these confounding factors, we need to take great measures in preparing our recording environment, as well as priming our participants on how to best behave during data collection. Luckily for us, we have a wealth of prior research involving the collection of EEG data. Thanks to this, we have access to a number of tools for data preprocessing and interpretation.
As for our specific hardware, we chose to use a headset called the EMOTIV EPOC X for our data collection. This specific headset has been used in many other studies and research projects because of its affordability, accuracy, and ease of use. There are 14 channels on the headset, capable of recording data from many different parts of the brain. Because we can adjust the locations of these electrodes, we can tailor our data collection to our individual needs. The main benefit of the Emotiv Epoc X headset is its plug-and-play nature, as well as the amount of documentation and sample data available.

	In terms of displaying our data we’ll be using generative art algorithms. Generative art is essentially any artwork that’s been created completely or in part by a non-human entity, and there is often a piece of the method that comes from either a random variable or outside system. The outside, random system for our generative art is the EEG data.
While simple plotting and graphs of the EEG data would work for many purposes, our goal is to represent it in a visually interesting way. So while we’ll be basing the skeleton of our final result on the input itself, we’ll also be using generative art techniques to spice it up a bit and add an element of randomness and unpredictability.
A recent example you may have seen that uses this generative art techniques is DALL·E, an AI that generates images based on a text prompt using machine learning. The technique has also been very prevalent in many brands of NFT over the last few years to make sure everyone has a different variation of the same type of image. At the moment, we’re experimenting with the use of harmonographic functions to display our information.

This is our web page so far. For our implementation, we decided to split the web service into two separate frontend and backend servers. This will allow us to easily test sections of our code, by separating art generation logic from the front end portion. On the right is the raw EEG data on a loop. We only recently received the headset and haven’t had a chance to use it to get our own data, so this is data from an online source. There are fourteen colored lines, each representative of the fourteen electrodes on the EMOTIV headset. The lines show the voltage at each of the electrodes. If you watch carefully, you’ll see spikes in different electrodes. This is potentially indicative of something occurring in the area of that particular electrode. 
To create this chart, we are using python. Its nature as an open-source language with a wide variety of libraries and documentation to draw on makes it perfect for a larger project with as many moving parts as this one has. For now, we’re using matplotlib as our method of choice for plotting the data. It’s a library for python that functions similarly to Matlab. We'll be using docker images to build the frontend, backend, and database servers separately to host on GCP or AWS. Using docker compose to create a cross platform testing environment for everyone on the team. This will mitigate any dependency problems from anyone's different computers and allow us to test in the exact same environment that would be in production.

On the right, you can see the harmonograph function being drawn. This is a version of generative art, however at the moment we have not integrated our EEG data into the artwork. Our idea is to be able to watch the artwork be drawn as the music is playing behind it.   We hope to implement multiple versions of generative art for every recording so people can choose their favorite piece after a session, as we develop our toolkit of different art techniques through these next two semesters. We'll be starting with simple techniques such as this harmonograph which is based off of two pendulums in both the x and y axis. These "pendulumns" are made from sin functions with exponential decay that all have different speeds, phases, and damping factors. This makes it easy to find the derivative of the line at each step and implement our EEG data by plotting the electrograph along the line. Showcasing your brainwaves in this harmonograph form from your favorite song. 
This is just the start for how complex these pieces will be, though. Soon we'll start processing the emotions and wave forms in tangum of the art. Looking at if someone is sad or happy, how focused they are, or if they're in deep memory retrieval. This way we can pair different art techniques and colors with these EEG artifacts so you can visibly see your brain's journey throughout the piece. We are also hoping in doing this that completely different art pieces will be generated for different genres of music. Comparing country to metal, classical to pop and so on, as everyone processes these genres differently. 
We're all very excited for what will come out of this and would like to thank you for listening to our presentation. If anyone is interested in participating in our project and having your brain recorded for your favorite song we've attached this QR code for you to scan. We'll want to record a lot of different people during this project so any help will be amazing and hopefully you'll get some really cool personalized art out of it too! Thank you!
