{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG Classification of Emotions Using CNN, SAE, and DNN\n",
    "\n",
    "From the paper \"EEG-Based Emotion Classification Using a Deep Neural Network and Sparse Autoencoder\", they found to have incredibly high recognition accuracy for emotions using EEG data that they put through a Convolutional Recurrent Neural Network, then a Sparse Autoencoder, and finally a Deep Neural Network.\n",
    "\n",
    "Paper: https://www.frontiersin.org/articles/10.3389/fnsys.2020.00043/full\n",
    "\n",
    "DEAP Dataset: https://www.eecs.qmul.ac.uk/mmv/datasets/deap/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "- 32 subjects\n",
    "- 40 sets of 63 second recordings (first 3 seconds are buffer)\n",
    "- 512 Hz recording\n",
    "- 32 EEG Channels\n",
    "- (30 * 40) 1280 recordings 32 * (512 * 63) 32256 matrixes\n",
    "- Valence and Arousal are recorded for all 40 trials of each subject\n",
    "\n",
    "Input Matrix Shape: `1280 x 32 x 32256`\n",
    "\n",
    "Output Matrix Shape: `1280 x 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Buffer Time\n",
    "\n",
    "- Remove the first 3 seconds of time from each epoch. We will not be normalizing.\n",
    "- (512 * 63) => (512 x 60)\n",
    "- 32256 => 30720\n",
    "\n",
    "Input Matrix Shape: `1280 x 32 x 30720`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decimate Data\n",
    "\n",
    "- Move from 512 Hz to 128 Hz\n",
    "- (512 * 60) => (128 * 60)\n",
    "- 30720 => 7680\n",
    "\n",
    "Input Matrix Shape: `1280 x 32 x 7680`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Band-Pass Filter\n",
    "\n",
    "- This is used to get rid of general noise\n",
    "- Done across the Hz range 0.1-50\n",
    "\n",
    "Input Matrix Shape: `1280 x 32 x 7680`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Butterworth Band-Pass Filter\n",
    "\n",
    "- $\\alpha$ (1-7 Hz), $\\beta$ (8-13 Hz), $\\theta$ (14-30 Hz), $\\lambda$ (30-45 Hz)\n",
    "- This will break up the single stream of data from each channel into 4 streams of separate bands\n",
    "\n",
    "Input Matrix Shape: `1280 x 4 x 32 x 7680`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch the Epochs\n",
    "\n",
    "- For each trils of 60 s, 14 segments are obtained using an 8 s time window moving every 4 s.\n",
    "- This will extrapolate our training data\n",
    "- (128 * 60) => (128 * 8)\n",
    "- 7680 => 1024\n",
    "- 1280 * 14 = 17920\n",
    "- Increasing our input data means we increase output\n",
    "- All 14 new samples from each apoch have the same output\n",
    "\n",
    "Input Matrix Shape: `17920 x 4 x 32 x 1024`\n",
    "\n",
    "Output Matrix Shape: `17920 x 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCC Feature Extraction\n",
    "\n",
    "- PCC (Pearson Correlation Coefficient)\n",
    "- On every band and epoch\n",
    "- Basically calculating correlation of every channel pair against each other\n",
    "- (32 x 1024) => (32 x 32)\n",
    "\n",
    "Input Matrix Shape: `17920 x 4 x 32 x 32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Training and Testing Set\n",
    "\n",
    "- 80% Training, 20% Testing\n",
    "\n",
    "Training Input Matrix Shape: `14336 x 4 x 32 x 32`\n",
    "\n",
    "Training Output Matrix Shape: `14336 x 2`\n",
    "\n",
    "Testing Input Matrix Shape: `3584 x 4 x 32 x 32`\n",
    "\n",
    "Testing Output Matrix Shape: `3584 x 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop Convolutional Neural Network (CNN)\n",
    "\n",
    "- Input Convolutional Layer\n",
    "    - Input Size: Four 32 x 32 \"images\" (4 bands PCC matrix)\n",
    "    - Kernel Size = 3 x 1\n",
    "    - \\# of Kernels = 32\n",
    "- Hidden Convolutional Layer\n",
    "    - Kernel Size = 3 x 1\n",
    "    - \\# of Kernels = 32\n",
    "- Dropout Layer\n",
    "- Hidden Max-Pooling Layer\n",
    "    - Pooling Size = 3 x 3\n",
    "    - \\# of Kernels = 64\n",
    "- Dropout Layer\n",
    "- Staging Layer\n",
    "    - Shape: 64 x 10 x 9\n",
    "- Output Layer\n",
    "    - (Valence, Arousal) tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN\n",
    "\n",
    "- Training Epochs = 50\n",
    "- Batch Size = 128\n",
    "- Learning Rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Training, Drop Output Layer\n",
    "\n",
    "- This makes the Staging Layer the new Output Layer\n",
    "- Run Training Input Matrix through CNN an get the output.\n",
    "- **Note:** Just using a CNN for classification is known to not have the best results. This is the main idea of the paper. We initially have a 2-neuron output layer for training in order for back propagation in the CNN to adjust the weights, but then dorp the output layer and take the data in the staging layer, flatten it, and put it into the SAE\n",
    "\n",
    "Training Input Matrix Size: `14336 x 64 x 10 x 9`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten 3D Output From CNN into 1D Vector\n",
    "\n",
    "- Data needs to be one-dimensional to go into SAE\n",
    "\n",
    "Training Input Matrix Size: `14336 x 5760`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop Sparse Autoencoder (SAE)\n",
    "\n",
    "- Input Layer: 5760 Nodes\n",
    "- Encoder Layer: 512 Nodes\n",
    "- Hidden Layer: 128 Nodes\n",
    "- Output Decoder Layer: 512 Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SAE\n",
    "\n",
    "- SAE's are unsupervised learning models so we don't train it against an expected output.\n",
    "- Training Epochs = 100\n",
    "- Batch Size = 64\n",
    "- Learning Rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get SAE Output\n",
    "\n",
    "- Using the flattened output from the CNN as input to the trained SAE, get the output to use as training data for DNN\n",
    "\n",
    "Training Input Matrix Shape: `14336 x 512`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop Deep Neural Network (DNN)\n",
    "\n",
    "- Fully Connected Input Layer: 512 Nodes\n",
    "- Fully Connected Hidden Layer: 256 Nodes\n",
    "- Fully Connected Output Layer: 2 Nodes ((Valence, Arousal) tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DNN\n",
    "\n",
    "- Use the output of the trained SAE as training input for the DNN. The expected outputs to the DNN are teh original (V,A) tuple.\n",
    "- Training Epochs = 100\n",
    "- Batch Size = 128\n",
    "- Learning Rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Full Model with Testing Set\n",
    "\n",
    "- `3584 x 4 x 32 x 32` ==CNN==> ==SAE==> ==DNN==> `3584 x 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
