{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15300 entries, 15300 to 30599\n",
      "Data columns (total 17 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   AF3           15300 non-null  float64\n",
      " 1   AF4           15300 non-null  float64\n",
      " 2   F3            15300 non-null  float64\n",
      " 3   F4            15300 non-null  float64\n",
      " 4   F7            15300 non-null  float64\n",
      " 5   F8            15300 non-null  float64\n",
      " 6   FC5           15300 non-null  float64\n",
      " 7   FC6           15300 non-null  float64\n",
      " 8   O1            15300 non-null  float64\n",
      " 9   O2            15300 non-null  float64\n",
      " 10  P7            15300 non-null  float64\n",
      " 11  P8            15300 non-null  float64\n",
      " 12  T7            15300 non-null  float64\n",
      " 13  T8            15300 non-null  float64\n",
      " 14  subject_num   15300 non-null  int64  \n",
      " 15  calc_valence  15300 non-null  float64\n",
      " 16  pred_valence  15300 non-null  int64  \n",
      "dtypes: float64(15), int64(2)\n",
      "memory usage: 2.0 MB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/f9chsbzs06z9v0zl61n4lws40000gn/T/ipykernel_36727/1225889147.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  emotions_pos[\"calc_valence\"] = ((emotions_pos[\"AF4\"]/emotions_pos[\"F4\"]) - (emotions_pos[\"AF3\"]/emotions_pos[\"F3\"]))\n",
      "/var/folders/0v/f9chsbzs06z9v0zl61n4lws40000gn/T/ipykernel_36727/1225889147.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  emotions_pos[\"pred_valence\"] = 2\n"
     ]
    }
   ],
   "source": [
    "n = 1\n",
    "# total = []\n",
    "emotions = \"../GAMEEMO/(S0{})/Preprocessed EEG Data/.csv format/S0{}G4AllChannels.csv\".format(n, n)\n",
    "data = pd.read_csv(emotions)\n",
    "data = data.drop(columns=[\"Unnamed: 14\"])\n",
    "data[\"subject_num\"] = n\n",
    "# third minute of each subject's data\n",
    "emotions_pos = data.iloc[15300:30600]\n",
    "\n",
    "# n=2\n",
    "# while n < 29:\n",
    "#     if n < 10: \n",
    "#         emotions = \"../GAMEEMO/(S0{})/Preprocessed EEG Data/.csv format/S0{}G4AllChannels.csv\".format(n, n)\n",
    "#     else:\n",
    "#         emotions = \"../GAMEEMO/(S{})/Preprocessed EEG Data/.csv format/S{}G4AllChannels.csv\".format(n, n)\n",
    "#     data = pd.read_csv(emotions)\n",
    "#     data = data.drop(columns=[\"Unnamed: 14\"])\n",
    "#     data[\"subject_num\"] = n\n",
    "#     emotions2 = data.iloc[22950:30600]\n",
    "#     emotions_pos = pd.concat([emotions_pos, emotions2], axis=0)\n",
    "#     # emotions1= emotions1.reset_index(drop=True)\n",
    "#     n = n+1\n",
    "# n = 2\n",
    "# emotions = \"../GAMEEMO/(S0{})/Preprocessed EEG Data/.csv format/S0{}G4AllChannels.csv\".format(n, n)\n",
    "# data = pd.read_csv(emotions)\n",
    "# data = data.drop(columns=[\"Unnamed: 14\"])\n",
    "# data[\"subject_num\"] = n\n",
    "# emotions2 = data.iloc[0:7650]\n",
    "# emotions1 = pd.concat([emotions1, emotions2], axis=0)\n",
    "# emotions1= emotions1.reset_index(drop=True)\n",
    "    \n",
    "emotions_pos[\"calc_valence\"] = ((emotions_pos[\"AF4\"]/emotions_pos[\"F4\"]) - (emotions_pos[\"AF3\"]/emotions_pos[\"F3\"]))\n",
    "emotions_pos[\"pred_valence\"] = 2\n",
    "# emotions_pos[\"pred_arousal\"] = 2\n",
    "print(emotions_pos.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15300 entries, 15300 to 30599\n",
      "Data columns (total 17 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   AF3           15300 non-null  float64\n",
      " 1   AF4           15300 non-null  float64\n",
      " 2   F3            15300 non-null  float64\n",
      " 3   F4            15300 non-null  float64\n",
      " 4   F7            15300 non-null  float64\n",
      " 5   F8            15300 non-null  float64\n",
      " 6   FC5           15300 non-null  float64\n",
      " 7   FC6           15300 non-null  float64\n",
      " 8   O1            15300 non-null  float64\n",
      " 9   O2            15300 non-null  float64\n",
      " 10  P7            15300 non-null  float64\n",
      " 11  P8            15300 non-null  float64\n",
      " 12  T7            15300 non-null  float64\n",
      " 13  T8            15300 non-null  float64\n",
      " 14  subject_num   15300 non-null  int64  \n",
      " 15  calc_valence  15300 non-null  float64\n",
      " 16  pred_valence  15300 non-null  int64  \n",
      "dtypes: float64(15), int64(2)\n",
      "memory usage: 2.0 MB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/f9chsbzs06z9v0zl61n4lws40000gn/T/ipykernel_36727/969571427.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  emotions_neg[\"calc_valence\"] = ((emotions_neg[\"AF4\"]/emotions_neg[\"F4\"]) - (emotions_neg[\"AF3\"]/emotions_neg[\"F3\"]))\n",
      "/var/folders/0v/f9chsbzs06z9v0zl61n4lws40000gn/T/ipykernel_36727/969571427.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  emotions_neg[\"pred_valence\"] = 1\n"
     ]
    }
   ],
   "source": [
    "n = 1\n",
    "# total = []\n",
    "emotions = \"../GAMEEMO/(S0{})/Preprocessed EEG Data/.csv format/S0{}G3AllChannels.csv\".format(n, n)\n",
    "data = pd.read_csv(emotions)\n",
    "data = data.drop(columns=[\"Unnamed: 14\"])\n",
    "data[\"subject_num\"] = n\n",
    "emotions_neg = data.iloc[15300:30600]\n",
    "\n",
    "# n = 2\n",
    "# while n < 29:\n",
    "#     if n < 10: \n",
    "#         emotions = \"../GAMEEMO/(S0{})/Preprocessed EEG Data/.csv format/S0{}G3AllChannels.csv\".format(n, n)\n",
    "#     else:\n",
    "#         emotions = \"../GAMEEMO/(S{})/Preprocessed EEG Data/.csv format/S{}G3AllChannels.csv\".format(n, n)\n",
    "#     data = pd.read_csv(emotions)\n",
    "#     data = data.drop(columns=[\"Unnamed: 14\"])\n",
    "#     data[\"subject_num\"] = n\n",
    "#     emotions2 = data.iloc[22950:30600]\n",
    "#     emotions_neg = pd.concat([emotions_neg, emotions2], axis=0)\n",
    "#     # emotions1= emotions1.reset_index(drop=True)\n",
    "#     n = n+1\n",
    "# n = 2\n",
    "# emotions = \"../GAMEEMO/(S0{})/Preprocessed EEG Data/.csv format/S0{}G4AllChannels.csv\".format(n, n)\n",
    "# data = pd.read_csv(emotions)\n",
    "# data = data.drop(columns=[\"Unnamed: 14\"])\n",
    "# data[\"subject_num\"] = n\n",
    "# emotions2 = data.iloc[0:7650]\n",
    "# emotions1 = pd.concat([emotions1, emotions2], axis=0)\n",
    "# emotions1= emotions1.reset_index(drop=True)\n",
    "emotions_neg[\"calc_valence\"] = ((emotions_neg[\"AF4\"]/emotions_neg[\"F4\"]) - (emotions_neg[\"AF3\"]/emotions_neg[\"F3\"]))\n",
    "emotions_neg[\"pred_valence\"] = 1\n",
    "# emotions_neg[\"pred_arousal\"] = 1\n",
    "\n",
    "print(emotions_neg.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            AF3       AF4        F3       F4        F7       F8       FC5  \\\n",
      "15300  10.58010  -4.07810   3.92120 -10.6829  22.41460   7.8255   7.10860   \n",
      "15301  -0.65968   0.50504 -10.41410   4.0581   6.96970  -6.7440  -7.41800   \n",
      "15302   3.71810   5.29510  -0.63053   8.5144   4.14100   7.7606   0.25762   \n",
      "15303  12.41320   4.25440  12.18160  -2.8426  10.88230  21.3950  14.34220   \n",
      "15304   3.71460  -7.81110   1.56890  -8.6975  -3.87060   6.0113  -0.61837   \n",
      "...         ...       ...       ...      ...       ...      ...       ...   \n",
      "30595   2.22500  20.49450  12.25930  29.7232  13.11810  34.8403  40.32050   \n",
      "30596 -12.00850   5.16490  -2.57620  13.8398  -1.76890  18.6499  23.80120   \n",
      "30597 -12.85480  -6.35320  -6.88090  -1.0906 -15.76280   3.4309   8.27320   \n",
      "30598  -6.53960   2.94630  -0.44227  13.0748  -0.71704  17.3250  21.87680   \n",
      "30599  -0.60433  11.20500   6.57390  22.2923  13.42600  30.3855  34.66420   \n",
      "\n",
      "            FC6       O1        O2       P7       P8       T7       T8  \\\n",
      "15300  -8.91480   1.2153 -14.31250  -5.4070  -8.9875   6.0368 -6.97370   \n",
      "15301  -0.66704  -9.9447   0.64627   1.1841  -4.1096  -3.0024  7.54470   \n",
      "15302   1.78300  -3.0814  11.57400   5.4515  -8.6839  -5.2325  0.22286   \n",
      "15303   1.43490   8.4315  -3.22040  -6.6861 -13.2244   0.1430 -6.78030   \n",
      "15304  10.74900   3.8284   0.94989   7.4537  -3.9949   9.5345 -4.68630   \n",
      "...         ...      ...       ...      ...      ...      ...      ...   \n",
      "30595   3.03290 -20.2306 -35.72950 -23.9909 -12.0974  11.7269  6.25680   \n",
      "30596   4.05590 -33.1168 -19.48570 -19.9003   2.7284  -2.7152 -6.89280   \n",
      "30597  17.91260 -45.2298  -4.21660  -8.5834  16.6647   4.1964  3.64340   \n",
      "30598  23.22530 -28.4160 -18.06360 -14.5759   1.5648   8.0424 -1.63640   \n",
      "30599   7.73180 -12.6110 -31.07980 -27.8013 -12.6291   8.2825  1.59500   \n",
      "\n",
      "       subject_num  calc_valence  pred_valence  \n",
      "15300            1     -2.316438             2  \n",
      "15301            1      0.061107             2  \n",
      "15302            1      6.518685             2  \n",
      "15303            1     -2.515670             2  \n",
      "15304            1     -1.469560             2  \n",
      "...            ...           ...           ...  \n",
      "30595            1      0.508017             1  \n",
      "30596            1     -4.288131             1  \n",
      "30597            1      3.957231             1  \n",
      "30598            1    -14.561101             1  \n",
      "30599            1      0.594569             1  \n",
      "\n",
      "[30600 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "emotions_all = pd.concat([emotions_pos, emotions_neg], axis=0)\n",
    "print(emotions_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Target Variable and Predictor Variables\n",
    "TargetVariable=['pred_valence']\n",
    "# Predictors=['AF3', 'AF4', 'F3', 'F4']\n",
    "Predictors=['AF3', 'AF4', 'F3', 'F4', 'F7', 'F8' ,'FC5', 'FC6', 'O1', 'O2', 'P7', 'P8', 'T7', 'T8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=emotions_all[Predictors].values\n",
    "y=emotions_all[TargetVariable].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sandardization of data ###\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "PredictorScaler=StandardScaler()\n",
    "TargetVarScaler=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the fit object for later reference\n",
    "PredictorScalerFit=PredictorScaler.fit(X)\n",
    "TargetVarScalerFit=TargetVarScaler.fit(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the standardized values of X and y\n",
    "X=PredictorScalerFit.transform(X)\n",
    "y=TargetVarScalerFit.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this clears dataframe\n",
    "# emotions_all = emotions_all.iloc[0:0]\n",
    "# print(emotions_neg.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21420, 14)\n",
      "(21420, 1)\n",
      "(9180, 14)\n",
      "(9180, 1)\n"
     ]
    }
   ],
   "source": [
    "# Quick sanity check with the shapes of Training and testing datasets\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 15:58:06.820550: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# importing the libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 15:58:19.071239: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# create ANN model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Input layer and FIRST hidden layer, both are same!\n",
    "model.add(Dense(units=5, input_dim=14, kernel_initializer='normal', activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Second layer of the model\n",
    "# after the first layer we don't have to specify input_dim as keras configure it automatically\n",
    "model.add(Dense(units=10, kernel_initializer='normal', activation='tanh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output neuron is a single fully connected node \n",
    "# Since we will be predicting a single number\n",
    "model.add(Dense(1, kernel_initializer='normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1071/1071 [==============================] - 5s 3ms/step - loss: 0.8713\n",
      "Epoch 2/50\n",
      "1071/1071 [==============================] - 3s 3ms/step - loss: 0.7627\n",
      "Epoch 3/50\n",
      "1071/1071 [==============================] - 4s 4ms/step - loss: 0.7508\n",
      "Epoch 4/50\n",
      "1071/1071 [==============================] - 3s 3ms/step - loss: 0.7465\n",
      "Epoch 5/50\n",
      "1071/1071 [==============================] - 4s 3ms/step - loss: 0.7430\n",
      "Epoch 6/50\n",
      "1071/1071 [==============================] - 3s 3ms/step - loss: 0.7414\n",
      "Epoch 7/50\n",
      "1071/1071 [==============================] - 3s 3ms/step - loss: 0.7391\n",
      "Epoch 8/50\n",
      "1071/1071 [==============================] - 5s 5ms/step - loss: 0.7372\n",
      "Epoch 9/50\n",
      "1071/1071 [==============================] - 3s 3ms/step - loss: 0.7376\n",
      "Epoch 10/50\n",
      "1071/1071 [==============================] - 3s 3ms/step - loss: 0.7359\n",
      "Epoch 11/50\n",
      "1071/1071 [==============================] - 3s 3ms/step - loss: 0.7360\n",
      "Epoch 12/50\n",
      "1071/1071 [==============================] - 3s 3ms/step - loss: 0.7353\n",
      "Epoch 13/50\n",
      "1071/1071 [==============================] - 3s 3ms/step - loss: 0.7358\n",
      "Epoch 14/50\n",
      "1071/1071 [==============================] - 4s 3ms/step - loss: 0.7342\n",
      "Epoch 15/50\n",
      "1071/1071 [==============================] - 3s 3ms/step - loss: 0.7347\n",
      "Epoch 16/50\n",
      "1071/1071 [==============================] - 4s 3ms/step - loss: 0.7343\n",
      "Epoch 17/50\n",
      "1071/1071 [==============================] - 3s 3ms/step - loss: 0.7335\n",
      "Epoch 18/50\n",
      "1071/1071 [==============================] - 2s 2ms/step - loss: 0.7330\n",
      "Epoch 19/50\n",
      "1071/1071 [==============================] - 2s 2ms/step - loss: 0.7333\n",
      "Epoch 20/50\n",
      "1071/1071 [==============================] - 4s 4ms/step - loss: 0.7333\n",
      "Epoch 21/50\n",
      "1071/1071 [==============================] - 5s 5ms/step - loss: 0.7330\n",
      "Epoch 22/50\n",
      "1071/1071 [==============================] - 6s 6ms/step - loss: 0.7324\n",
      "Epoch 23/50\n",
      "1071/1071 [==============================] - 8s 7ms/step - loss: 0.7326\n",
      "Epoch 24/50\n",
      "1071/1071 [==============================] - 9s 8ms/step - loss: 0.7319\n",
      "Epoch 25/50\n",
      "1071/1071 [==============================] - 9s 8ms/step - loss: 0.7315\n",
      "Epoch 26/50\n",
      "1071/1071 [==============================] - 5s 4ms/step - loss: 0.7318\n",
      "Epoch 27/50\n",
      "1071/1071 [==============================] - 3s 3ms/step - loss: 0.7303\n",
      "Epoch 28/50\n",
      "1071/1071 [==============================] - 3s 3ms/step - loss: 0.7318\n",
      "Epoch 29/50\n",
      "1071/1071 [==============================] - 4s 3ms/step - loss: 0.7308\n",
      "Epoch 30/50\n",
      "1071/1071 [==============================] - 3s 3ms/step - loss: 0.7313\n",
      "Epoch 31/50\n",
      "1071/1071 [==============================] - 4s 4ms/step - loss: 0.7303\n",
      "Epoch 32/50\n",
      "1071/1071 [==============================] - 3s 2ms/step - loss: 0.7304\n",
      "Epoch 33/50\n",
      "1071/1071 [==============================] - 4s 3ms/step - loss: 0.7300\n",
      "Epoch 34/50\n",
      "1071/1071 [==============================] - 2s 2ms/step - loss: 0.7299\n",
      "Epoch 35/50\n",
      "1071/1071 [==============================] - 5s 4ms/step - loss: 0.7295\n",
      "Epoch 36/50\n",
      "1071/1071 [==============================] - 4s 4ms/step - loss: 0.7293\n",
      "Epoch 37/50\n",
      "1071/1071 [==============================] - 4s 4ms/step - loss: 0.7285\n",
      "Epoch 38/50\n",
      "1071/1071 [==============================] - 3s 3ms/step - loss: 0.7288\n",
      "Epoch 39/50\n",
      "1071/1071 [==============================] - 4s 4ms/step - loss: 0.7289\n",
      "Epoch 40/50\n",
      "1071/1071 [==============================] - 5s 4ms/step - loss: 0.7291\n",
      "Epoch 41/50\n",
      "1071/1071 [==============================] - 3s 3ms/step - loss: 0.7275\n",
      "Epoch 42/50\n",
      "1071/1071 [==============================] - 4s 4ms/step - loss: 0.7287\n",
      "Epoch 43/50\n",
      "1071/1071 [==============================] - 5s 5ms/step - loss: 0.7271\n",
      "Epoch 44/50\n",
      "1071/1071 [==============================] - 5s 5ms/step - loss: 0.7279\n",
      "Epoch 45/50\n",
      "1071/1071 [==============================] - 4s 4ms/step - loss: 0.7274\n",
      "Epoch 46/50\n",
      "1071/1071 [==============================] - 4s 3ms/step - loss: 0.7275\n",
      "Epoch 47/50\n",
      "1071/1071 [==============================] - 6s 6ms/step - loss: 0.7268\n",
      "Epoch 48/50\n",
      "1071/1071 [==============================] - 5s 4ms/step - loss: 0.7273\n",
      "Epoch 49/50\n",
      "1071/1071 [==============================] - 3s 3ms/step - loss: 0.7274\n",
      "Epoch 50/50\n",
      "1071/1071 [==============================] - 3s 3ms/step - loss: 0.7278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12b103c40>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "model.fit(X_train, y_train ,batch_size = 20, epochs = 50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to find the best parameters for ANN\n",
    "def FunctionFindBestParams(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Defining the list of hyper parameters to try\n",
    "    batch_size_list=[10]\n",
    "    epoch_list  =   [10]\n",
    "    \n",
    "    import pandas as pd\n",
    "    SearchResultsData=pd.DataFrame(columns=['TrialNumber', 'Parameters', 'Accuracy'])\n",
    "    \n",
    "    # initializing the trials\n",
    "    TrialNumber=0\n",
    "    for batch_size_trial in batch_size_list:\n",
    "        for epochs_trial in epoch_list:\n",
    "            TrialNumber+=1\n",
    "            # create ANN model\n",
    "            model = Sequential()\n",
    "            # Defining the first layer of the model\n",
    "            model.add(Dense(units=5, input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    " \n",
    "            # Defining the Second layer of the model\n",
    "            model.add(Dense(units=10, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "            # Defining the third layer of the model\n",
    "            model.add(Dense(units=5, kernel_initializer='normal', activation='relu'))\n",
    " \n",
    "            # The output neuron is a single fully connected node \n",
    "            # Since we will be predicting a single number\n",
    "            model.add(Dense(1, kernel_initializer='normal'))\n",
    " \n",
    "            # Compiling the model\n",
    "            model.compile(loss='mean_squared_error', optimizer='adam')\n",
    " \n",
    "            # Fitting the ANN to the Training set\n",
    "            model.fit(X_train, y_train ,batch_size = batch_size_trial, epochs = epochs_trial, verbose=0)\n",
    " \n",
    "            # Mean absolute percentage error\n",
    "            MAPE = mean_absolute_percentage_error(y_test, model.predict(X_test))\n",
    "            \n",
    "            # printing the results of the current iteration\n",
    "            print(TrialNumber, 'Parameters:','batch_size:', batch_size_trial,'-', 'epochs:',epochs_trial, 'Accuracy:', 100-MAPE)\n",
    "            \n",
    "            SearchResultsData=SearchResultsData.append(pd.DataFrame(data=[[TrialNumber, str(batch_size_trial)+'-'+str(epochs_trial), 100-MAPE]],\n",
    "                                                                    columns=['TrialNumber', 'Parameters', 'Accuracy'] ))\n",
    "    return(SearchResultsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287/287 [==============================] - 1s 4ms/step\n",
      "1 Parameters: batch_size: 10 - epochs: 10 Accuracy: 99.00003462017662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/f9chsbzs06z9v0zl61n4lws40000gn/T/ipykernel_36727/493997042.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  SearchResultsData=SearchResultsData.append(pd.DataFrame(data=[[TrialNumber, str(batch_size_trial)+'-'+str(epochs_trial), 100-MAPE]],\n"
     ]
    }
   ],
   "source": [
    "# Calling the function\n",
    "ResultsData=FunctionFindBestParams(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: xlabel='Parameters'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "ResultsData.plot(x='Parameters', y='Accuracy', figsize=(15,4), kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287/287 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AF3</th>\n",
       "      <th>AF4</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>FC5</th>\n",
       "      <th>FC6</th>\n",
       "      <th>O1</th>\n",
       "      <th>O2</th>\n",
       "      <th>P7</th>\n",
       "      <th>P8</th>\n",
       "      <th>T7</th>\n",
       "      <th>T8</th>\n",
       "      <th>Valence</th>\n",
       "      <th>PredictedValence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.174500</td>\n",
       "      <td>10.8847</td>\n",
       "      <td>2.5957</td>\n",
       "      <td>5.1502</td>\n",
       "      <td>8.1728</td>\n",
       "      <td>-2.2302</td>\n",
       "      <td>0.61602</td>\n",
       "      <td>20.4918</td>\n",
       "      <td>-3.5820</td>\n",
       "      <td>30.5451</td>\n",
       "      <td>14.7252</td>\n",
       "      <td>16.13910</td>\n",
       "      <td>-9.83290</td>\n",
       "      <td>-0.59193</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.374697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-11.153300</td>\n",
       "      <td>2.3705</td>\n",
       "      <td>-12.9679</td>\n",
       "      <td>1.2761</td>\n",
       "      <td>-25.8488</td>\n",
       "      <td>-24.4377</td>\n",
       "      <td>-14.27160</td>\n",
       "      <td>16.8480</td>\n",
       "      <td>-25.7301</td>\n",
       "      <td>14.6013</td>\n",
       "      <td>13.7486</td>\n",
       "      <td>14.12090</td>\n",
       "      <td>-5.84000</td>\n",
       "      <td>4.09570</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.940660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.245600</td>\n",
       "      <td>10.2649</td>\n",
       "      <td>10.4520</td>\n",
       "      <td>-0.5282</td>\n",
       "      <td>9.1995</td>\n",
       "      <td>10.8041</td>\n",
       "      <td>16.12800</td>\n",
       "      <td>-3.5700</td>\n",
       "      <td>2.8007</td>\n",
       "      <td>-17.2152</td>\n",
       "      <td>-16.0575</td>\n",
       "      <td>-0.17754</td>\n",
       "      <td>1.98040</td>\n",
       "      <td>-6.79820</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.895932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.064811</td>\n",
       "      <td>-3.0101</td>\n",
       "      <td>2.1468</td>\n",
       "      <td>-9.8264</td>\n",
       "      <td>226.1056</td>\n",
       "      <td>-13.5696</td>\n",
       "      <td>-12.43260</td>\n",
       "      <td>4.7820</td>\n",
       "      <td>-3.1431</td>\n",
       "      <td>-12.8444</td>\n",
       "      <td>-1.9954</td>\n",
       "      <td>-3.56100</td>\n",
       "      <td>0.83524</td>\n",
       "      <td>-7.82380</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.912090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.933200</td>\n",
       "      <td>2.7788</td>\n",
       "      <td>11.9974</td>\n",
       "      <td>20.5833</td>\n",
       "      <td>17.9068</td>\n",
       "      <td>-6.5949</td>\n",
       "      <td>14.52390</td>\n",
       "      <td>1.4327</td>\n",
       "      <td>-17.6823</td>\n",
       "      <td>-13.9791</td>\n",
       "      <td>-27.3836</td>\n",
       "      <td>-8.04760</td>\n",
       "      <td>-12.45800</td>\n",
       "      <td>-9.86000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.510953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         AF3      AF4       F3       F4        F7       F8       FC5      FC6  \\\n",
       "0   3.174500  10.8847   2.5957   5.1502    8.1728  -2.2302   0.61602  20.4918   \n",
       "1 -11.153300   2.3705 -12.9679   1.2761  -25.8488 -24.4377 -14.27160  16.8480   \n",
       "2   5.245600  10.2649  10.4520  -0.5282    9.1995  10.8041  16.12800  -3.5700   \n",
       "3   0.064811  -3.0101   2.1468  -9.8264  226.1056 -13.5696 -12.43260   4.7820   \n",
       "4   5.933200   2.7788  11.9974  20.5833   17.9068  -6.5949  14.52390   1.4327   \n",
       "\n",
       "        O1       O2       P7        P8        T7       T8  Valence  \\\n",
       "0  -3.5820  30.5451  14.7252  16.13910  -9.83290 -0.59193      2.0   \n",
       "1 -25.7301  14.6013  13.7486  14.12090  -5.84000  4.09570      2.0   \n",
       "2   2.8007 -17.2152 -16.0575  -0.17754   1.98040 -6.79820      2.0   \n",
       "3  -3.1431 -12.8444  -1.9954  -3.56100   0.83524 -7.82380      1.0   \n",
       "4 -17.6823 -13.9791 -27.3836  -8.04760 -12.45800 -9.86000      1.0   \n",
       "\n",
       "   PredictedValence  \n",
       "0          1.374697  \n",
       "1          1.940660  \n",
       "2          1.895932  \n",
       "3          0.912090  \n",
       "4          1.510953  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Fitting the ANN to the Training set\n",
    "model.fit(X_train, y_train ,batch_size = 25, epochs = 50, verbose=0)\n",
    " \n",
    "# Generating Predictions on testing data\n",
    "Predictions=model.predict(X_test)\n",
    " \n",
    "# Scaling the predicted Price data back to original price scale\n",
    "Predictions=TargetVarScalerFit.inverse_transform(Predictions)\n",
    " \n",
    "# Scaling the y_test Price data back to original price scale\n",
    "y_test_orig=TargetVarScalerFit.inverse_transform(y_test)\n",
    " \n",
    "# Scaling the test data back to original scale\n",
    "Test_Data=PredictorScalerFit.inverse_transform(X_test)\n",
    " \n",
    "TestingData=pd.DataFrame(data=Test_Data, columns=Predictors)\n",
    "TestingData['Valence']=y_test_orig\n",
    "TestingData['PredictedValence']=Predictions\n",
    "TestingData.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of ANN model is: 73.29920931319526\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AF3</th>\n",
       "      <th>AF4</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>FC5</th>\n",
       "      <th>FC6</th>\n",
       "      <th>O1</th>\n",
       "      <th>O2</th>\n",
       "      <th>P7</th>\n",
       "      <th>P8</th>\n",
       "      <th>T7</th>\n",
       "      <th>T8</th>\n",
       "      <th>Valence</th>\n",
       "      <th>PredictedValence</th>\n",
       "      <th>APE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.174500</td>\n",
       "      <td>10.8847</td>\n",
       "      <td>2.5957</td>\n",
       "      <td>5.1502</td>\n",
       "      <td>8.1728</td>\n",
       "      <td>-2.2302</td>\n",
       "      <td>0.61602</td>\n",
       "      <td>20.4918</td>\n",
       "      <td>-3.5820</td>\n",
       "      <td>30.5451</td>\n",
       "      <td>14.7252</td>\n",
       "      <td>16.13910</td>\n",
       "      <td>-9.83290</td>\n",
       "      <td>-0.59193</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.374697</td>\n",
       "      <td>31.265128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-11.153300</td>\n",
       "      <td>2.3705</td>\n",
       "      <td>-12.9679</td>\n",
       "      <td>1.2761</td>\n",
       "      <td>-25.8488</td>\n",
       "      <td>-24.4377</td>\n",
       "      <td>-14.27160</td>\n",
       "      <td>16.8480</td>\n",
       "      <td>-25.7301</td>\n",
       "      <td>14.6013</td>\n",
       "      <td>13.7486</td>\n",
       "      <td>14.12090</td>\n",
       "      <td>-5.84000</td>\n",
       "      <td>4.09570</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.940660</td>\n",
       "      <td>2.967000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.245600</td>\n",
       "      <td>10.2649</td>\n",
       "      <td>10.4520</td>\n",
       "      <td>-0.5282</td>\n",
       "      <td>9.1995</td>\n",
       "      <td>10.8041</td>\n",
       "      <td>16.12800</td>\n",
       "      <td>-3.5700</td>\n",
       "      <td>2.8007</td>\n",
       "      <td>-17.2152</td>\n",
       "      <td>-16.0575</td>\n",
       "      <td>-0.17754</td>\n",
       "      <td>1.98040</td>\n",
       "      <td>-6.79820</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.895932</td>\n",
       "      <td>5.203378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.064811</td>\n",
       "      <td>-3.0101</td>\n",
       "      <td>2.1468</td>\n",
       "      <td>-9.8264</td>\n",
       "      <td>226.1056</td>\n",
       "      <td>-13.5696</td>\n",
       "      <td>-12.43260</td>\n",
       "      <td>4.7820</td>\n",
       "      <td>-3.1431</td>\n",
       "      <td>-12.8444</td>\n",
       "      <td>-1.9954</td>\n",
       "      <td>-3.56100</td>\n",
       "      <td>0.83524</td>\n",
       "      <td>-7.82380</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.912090</td>\n",
       "      <td>8.791047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.933200</td>\n",
       "      <td>2.7788</td>\n",
       "      <td>11.9974</td>\n",
       "      <td>20.5833</td>\n",
       "      <td>17.9068</td>\n",
       "      <td>-6.5949</td>\n",
       "      <td>14.52390</td>\n",
       "      <td>1.4327</td>\n",
       "      <td>-17.6823</td>\n",
       "      <td>-13.9791</td>\n",
       "      <td>-27.3836</td>\n",
       "      <td>-8.04760</td>\n",
       "      <td>-12.45800</td>\n",
       "      <td>-9.86000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.510953</td>\n",
       "      <td>51.095343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         AF3      AF4       F3       F4        F7       F8       FC5      FC6  \\\n",
       "0   3.174500  10.8847   2.5957   5.1502    8.1728  -2.2302   0.61602  20.4918   \n",
       "1 -11.153300   2.3705 -12.9679   1.2761  -25.8488 -24.4377 -14.27160  16.8480   \n",
       "2   5.245600  10.2649  10.4520  -0.5282    9.1995  10.8041  16.12800  -3.5700   \n",
       "3   0.064811  -3.0101   2.1468  -9.8264  226.1056 -13.5696 -12.43260   4.7820   \n",
       "4   5.933200   2.7788  11.9974  20.5833   17.9068  -6.5949  14.52390   1.4327   \n",
       "\n",
       "        O1       O2       P7        P8        T7       T8  Valence  \\\n",
       "0  -3.5820  30.5451  14.7252  16.13910  -9.83290 -0.59193      2.0   \n",
       "1 -25.7301  14.6013  13.7486  14.12090  -5.84000  4.09570      2.0   \n",
       "2   2.8007 -17.2152 -16.0575  -0.17754   1.98040 -6.79820      2.0   \n",
       "3  -3.1431 -12.8444  -1.9954  -3.56100   0.83524 -7.82380      1.0   \n",
       "4 -17.6823 -13.9791 -27.3836  -8.04760 -12.45800 -9.86000      1.0   \n",
       "\n",
       "   PredictedValence        APE  \n",
       "0          1.374697  31.265128  \n",
       "1          1.940660   2.967000  \n",
       "2          1.895932   5.203378  \n",
       "3          0.912090   8.791047  \n",
       "4          1.510953  51.095343  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Computing the absolute percent error\n",
    "APE=100*(abs(TestingData['Valence']-TestingData['PredictedValence'])/TestingData['Valence'])\n",
    "TestingData['APE']=APE\n",
    " \n",
    "print('The Accuracy of ANN model is:', 100-np.mean(APE))\n",
    "TestingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            AF3       AF4       F3        F4        F7        F8       FC5  \\\n",
      "0      3.174500  10.88470   2.5957   5.15020    8.1728  -2.23020   0.61602   \n",
      "1    -11.153300   2.37050 -12.9679   1.27610  -25.8488 -24.43770 -14.27160   \n",
      "2      5.245600  10.26490  10.4520  -0.52820    9.1995  10.80410  16.12800   \n",
      "3      0.064811  -3.01010   2.1468  -9.82640  226.1056 -13.56960 -12.43260   \n",
      "4      5.933200   2.77880  11.9974  20.58330   17.9068  -6.59490  14.52390   \n",
      "...         ...       ...      ...       ...       ...       ...       ...   \n",
      "9175  -6.287700   5.79260  -3.8075  -3.33610  -12.2008 -10.28030 -17.94190   \n",
      "9176  -2.439000  -6.26780  -4.3720   0.24169   -4.8878   2.71340  -6.22720   \n",
      "9177  -0.153270  -6.99420  -1.6008  -4.40310    9.8009   0.65683  -6.59080   \n",
      "9178   4.385800   1.99690   5.1114  13.66010   -2.3640  17.24770   5.58610   \n",
      "9179  -0.579050  -0.13412   2.1850   0.52757    7.3237  -9.79910  -4.16840   \n",
      "\n",
      "            FC6       O1       O2       P7        P8        T7        T8  \\\n",
      "0     20.491800  -3.5820  30.5451  14.7252  16.13910  -9.83290  -0.59193   \n",
      "1     16.848000 -25.7301  14.6013  13.7486  14.12090  -5.84000   4.09570   \n",
      "2     -3.570000   2.8007 -17.2152 -16.0575  -0.17754   1.98040  -6.79820   \n",
      "3      4.782000  -3.1431 -12.8444  -1.9954  -3.56100   0.83524  -7.82380   \n",
      "4      1.432700 -17.6823 -13.9791 -27.3836  -8.04760 -12.45800  -9.86000   \n",
      "...         ...      ...      ...      ...       ...       ...       ...   \n",
      "9175  14.532200  -7.4050  19.8559  12.0535  24.77900  -1.56630   4.04080   \n",
      "9176   0.012674  -4.5888   4.1907  11.3685  -1.72300  -4.20920   6.80290   \n",
      "9177   8.645500   6.5133   9.4042   2.9229   1.13360  10.07570  -4.73150   \n",
      "9178  -7.550200  -9.5550 -10.6285 -16.8645 -14.20930  -3.36290  12.21970   \n",
      "9179   1.471000  -2.9545   4.6887  -2.2389   7.18180  10.19790  -5.86890   \n",
      "\n",
      "      Valence  PredictedValence        APE  \n",
      "0         2.0          1.374697  31.265128  \n",
      "1         2.0          1.940660   2.967000  \n",
      "2         2.0          1.895932   5.203378  \n",
      "3         1.0          0.912090   8.791047  \n",
      "4         1.0          1.510953  51.095343  \n",
      "...       ...               ...        ...  \n",
      "9175      2.0          1.411374  29.431295  \n",
      "9176      2.0          1.473224  26.338822  \n",
      "9177      2.0          1.378634  31.068295  \n",
      "9178      1.0          1.079242   7.924151  \n",
      "9179      1.0          1.163929  16.392851  \n",
      "\n",
      "[9180 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "print(TestingData)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
